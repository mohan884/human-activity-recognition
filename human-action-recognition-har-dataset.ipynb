{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3733921,"sourceType":"datasetVersion","datasetId":2232355}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T22:03:26.148349Z","iopub.execute_input":"2024-01-11T22:03:26.148745Z","iopub.status.idle":"2024-01-11T22:03:38.628916Z","shell.execute_reply.started":"2024-01-11T22:03:26.148711Z","shell.execute_reply":"2024-01-11T22:03:38.627913Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1| what is HAR\n* Activity Recognition aims at identifying the activity of users based on series ofobservations collected during the activity in a definite context environment. Appli-cations that are enabled with activity recognition are gaining huge attention, asusers get personalized services and support based on their contextual behavior. Theproliferation of wearable devices and smartphones has provided real-time monitor-ing of human activities through sensors that are embedded in smart devices such asproximity sensors, cameras, microphone, magnetometers accelerometers, gyro-scopes, GPS etc., Thus, understanding human activities in inferring the gesture orposition has created a competitive challenge in building personal health care systems, examining wellness and fit characteristics, and most pre-dominantly inelderly care, abnormal activity detection, diabetes or epilepsy disorders etc.,Thus, Human Activity Recognition (HAR) plays a significant part in enhancingpeople’s lifestyle, as it should be competent enough in learning high level qualityinformation from raw sensor data. Effective HAR applications are incorporated forcontextual behavior analysis [1], video surveillance analysis [1], gait investigation(to determine any abnormalities in walking or running), gesture and positionrecognition. \n\n\n* Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data.\n* Movements are often typical activities performed indoors, such as walking, talking, standing, and sitting\n\n\n# Why it is important ?\n* Human activity recognition plays a significant role in human-to-human interaction and interpersonal relations.\n* Because it provides information about the identity of a person, their personality, and psychological state, it is difficult to extract.\n* The human ability to recognize another person’s activities is one of the main subjects of study of the scientific areas of computer vision and machine learning. As a result of this research, many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system.","metadata":{}},{"cell_type":"markdown","source":"# 2| Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Activation, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\n\nfrom tensorflow.keras.utils import to_categorical\n\nimport seaborn as sns\nimport matplotlib.image as img\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:14:27.868899Z","iopub.execute_input":"2024-01-11T22:14:27.869261Z","iopub.status.idle":"2024-01-11T22:14:28.017199Z","shell.execute_reply.started":"2024-01-11T22:14:27.869236Z","shell.execute_reply":"2024-01-11T22:14:28.01625Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3| Getting the path and Loading the data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/human-action-recognition-har-dataset/Human Action Recognition/Training_set.csv\")\ntest_data = pd.read_csv(\"../input/human-action-recognition-har-dataset/Human Action Recognition/Testing_set.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:14:44.2613Z","iopub.execute_input":"2024-01-11T22:14:44.26215Z","iopub.status.idle":"2024-01-11T22:14:44.282774Z","shell.execute_reply.started":"2024-01-11T22:14:44.262117Z","shell.execute_reply":"2024-01-11T22:14:44.282003Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_fol = glob.glob(\"../input/human-action-recognition-har-dataset/Human Action Recognition/train/*\") \ntest_fol = glob.glob(\"../input/human-action-recognition-har-dataset/Human Action Recognition/test/*\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:14:49.654239Z","iopub.execute_input":"2024-01-11T22:14:49.655157Z","iopub.status.idle":"2024-01-11T22:14:49.720266Z","shell.execute_reply.started":"2024-01-11T22:14:49.655126Z","shell.execute_reply":"2024-01-11T22:14:49.719302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:14:58.953972Z","iopub.execute_input":"2024-01-11T22:14:58.954598Z","iopub.status.idle":"2024-01-11T22:14:58.966431Z","shell.execute_reply.started":"2024-01-11T22:14:58.954568Z","shell.execute_reply":"2024-01-11T22:14:58.965284Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:15:11.419783Z","iopub.execute_input":"2024-01-11T22:15:11.420153Z","iopub.status.idle":"2024-01-11T22:15:11.429603Z","shell.execute_reply.started":"2024-01-11T22:15:11.420125Z","shell.execute_reply":"2024-01-11T22:15:11.4287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nHAR = train_data.label.value_counts()\nfig = px.pie(train_data, values=HAR.values, names=HAR.index, title='Distribution of Human Activity')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:15:56.034271Z","iopub.execute_input":"2024-01-11T22:15:56.035Z","iopub.status.idle":"2024-01-11T22:15:56.08957Z","shell.execute_reply.started":"2024-01-11T22:15:56.034966Z","shell.execute_reply":"2024-01-11T22:15:56.088597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filename = train_data['filename']\n\nsituation = train_data['label']","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:16:10.587427Z","iopub.execute_input":"2024-01-11T22:16:10.588301Z","iopub.status.idle":"2024-01-11T22:16:10.592625Z","shell.execute_reply.started":"2024-01-11T22:16:10.588264Z","shell.execute_reply":"2024-01-11T22:16:10.591698Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filename","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:16:18.011025Z","iopub.execute_input":"2024-01-11T22:16:18.011412Z","iopub.status.idle":"2024-01-11T22:16:18.019906Z","shell.execute_reply.started":"2024-01-11T22:16:18.011382Z","shell.execute_reply":"2024-01-11T22:16:18.018873Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4| Making function that take random path and display the image","metadata":{}},{"cell_type":"code","source":"def displaying_random_images():\n    num = random.randint(1,10000)\n    imgg = \"Image_{}.jpg\".format(num)\n    train = \"../input/human-action-recognition-har-dataset/Human Action Recognition/train/\"\n    if os.path.exists(train+imgg):\n        testImage = img.imread(train+imgg)\n        plt.imshow(testImage)\n        plt.title(\"{}\".format(train_data.loc[train_data['filename'] == \"{}\".format(imgg), 'label'].item()))\n\n    else:\n        #print(train+img)\n        print(\"File Path not found \\nSkipping the file!!\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:21:18.967304Z","iopub.execute_input":"2024-01-11T22:21:18.967708Z","iopub.status.idle":"2024-01-11T22:21:18.974521Z","shell.execute_reply.started":"2024-01-11T22:21:18.967679Z","shell.execute_reply":"2024-01-11T22:21:18.973338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:21:22.150944Z","iopub.execute_input":"2024-01-11T22:21:22.151306Z","iopub.status.idle":"2024-01-11T22:21:22.63672Z","shell.execute_reply.started":"2024-01-11T22:21:22.151278Z","shell.execute_reply":"2024-01-11T22:21:22.634447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:18:07.458242Z","iopub.execute_input":"2024-01-11T22:18:07.459145Z","iopub.status.idle":"2024-01-11T22:18:07.800585Z","shell.execute_reply.started":"2024-01-11T22:18:07.459109Z","shell.execute_reply":"2024-01-11T22:18:07.799603Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:18:13.05452Z","iopub.execute_input":"2024-01-11T22:18:13.054908Z","iopub.status.idle":"2024-01-11T22:18:13.432241Z","shell.execute_reply.started":"2024-01-11T22:18:13.05488Z","shell.execute_reply":"2024-01-11T22:18:13.431305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:18:17.553322Z","iopub.execute_input":"2024-01-11T22:18:17.553734Z","iopub.status.idle":"2024-01-11T22:18:18.114644Z","shell.execute_reply.started":"2024-01-11T22:18:17.553704Z","shell.execute_reply":"2024-01-11T22:18:18.11373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5| Data preprocessing","metadata":{}},{"cell_type":"code","source":"img_data = []\nimg_label = []\nlength = len(train_fol)\nfor i in (range(len(train_fol)-1)):\n    t = '../input/human-action-recognition-har-dataset/Human Action Recognition/train/' + filename[i]    \n    temp_img = Image.open(t)\n    img_data.append(np.asarray(temp_img.resize((160,160))))\n    img_label.append(situation[i])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:23:29.404772Z","iopub.execute_input":"2024-01-11T22:23:29.40518Z","iopub.status.idle":"2024-01-11T22:24:52.18708Z","shell.execute_reply.started":"2024-01-11T22:23:29.405146Z","shell.execute_reply":"2024-01-11T22:24:52.185933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_shape= (160,160,3)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:25:36.549714Z","iopub.execute_input":"2024-01-11T22:25:36.550293Z","iopub.status.idle":"2024-01-11T22:25:36.554436Z","shell.execute_reply.started":"2024-01-11T22:25:36.550264Z","shell.execute_reply":"2024-01-11T22:25:36.553482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"iii = img_data\niii = np.asarray(iii)\ntype(iii)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:31:34.207929Z","iopub.execute_input":"2024-01-11T22:31:34.208312Z","iopub.status.idle":"2024-01-11T22:31:34.521243Z","shell.execute_reply.started":"2024-01-11T22:31:34.208281Z","shell.execute_reply":"2024-01-11T22:31:34.520235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = to_categorical(np.asarray(train_data[\"label\"].factorize()[0]))\nprint(y_train[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:26:31.457425Z","iopub.execute_input":"2024-01-11T22:26:31.457812Z","iopub.status.idle":"2024-01-11T22:26:31.466508Z","shell.execute_reply.started":"2024-01-11T22:26:31.457785Z","shell.execute_reply":"2024-01-11T22:26:31.465206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6| Make an CNN model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficientnet_model = Sequential()\n\nmodel = tf.keras.applications.EfficientNetB7(include_top=False,\n                                            input_shape=(160,160,3),\n                                            pooling =\"avg\",classes=15,\n                                             weights=\"imagenet\")\n\nfor layer in model.layers:\n    layer.trainable=False\n    \n\nefficientnet_model.add(model)\nefficientnet_model.add(Flatten())\nefficientnet_model.add(Dense(512,activation=\"relu\"))\nefficientnet_model.add(Dense(15,activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:29:52.338415Z","iopub.execute_input":"2024-01-11T22:29:52.338849Z","iopub.status.idle":"2024-01-11T22:30:15.568325Z","shell.execute_reply.started":"2024-01-11T22:29:52.338818Z","shell.execute_reply":"2024-01-11T22:30:15.56688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficientnet_model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:30:57.343388Z","iopub.execute_input":"2024-01-11T22:30:57.343759Z","iopub.status.idle":"2024-01-11T22:30:57.3799Z","shell.execute_reply.started":"2024-01-11T22:30:57.343732Z","shell.execute_reply":"2024-01-11T22:30:57.379011Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficientnet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:31:06.865449Z","iopub.execute_input":"2024-01-11T22:31:06.865838Z","iopub.status.idle":"2024-01-11T22:31:06.951669Z","shell.execute_reply.started":"2024-01-11T22:31:06.865808Z","shell.execute_reply":"2024-01-11T22:31:06.950737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = efficientnet_model.fit(iii,y_train,epochs=40)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T22:31:55.717778Z","iopub.execute_input":"2024-01-11T22:31:55.718623Z","iopub.status.idle":"2024-01-11T23:05:48.70886Z","shell.execute_reply.started":"2024-01-11T22:31:55.718585Z","shell.execute_reply":"2024-01-11T23:05:48.708029Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"losses = history.history[\"loss\"]\nplt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:06:22.197986Z","iopub.execute_input":"2024-01-11T23:06:22.198345Z","iopub.status.idle":"2024-01-11T23:06:22.601479Z","shell.execute_reply.started":"2024-01-11T23:06:22.198316Z","shell.execute_reply":"2024-01-11T23:06:22.600503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = history.history['accuracy']\nplt.plot(acc)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:06:43.141313Z","iopub.execute_input":"2024-01-11T23:06:43.142363Z","iopub.status.idle":"2024-01-11T23:06:43.554358Z","shell.execute_reply.started":"2024-01-11T23:06:43.142324Z","shell.execute_reply":"2024-01-11T23:06:43.553431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7| Model predictions","metadata":{}},{"cell_type":"code","source":"def read_img(fn):\n    img = Image.open(fn)\n    return np.asarray(img.resize((160,160)))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:07:37.024066Z","iopub.execute_input":"2024-01-11T23:07:37.024478Z","iopub.status.idle":"2024-01-11T23:07:37.03064Z","shell.execute_reply.started":"2024-01-11T23:07:37.024445Z","shell.execute_reply":"2024-01-11T23:07:37.029608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_predict(test_image):\n    result = efficientnet_model.predict(np.asarray([read_img(test_image)]))\n\n    itemindex = np.where(result==np.max(result))\n    prediction = itemindex[1][0]\n    print(\"probability: \"+str(np.max(result)*100) + \"%\\nPredicted class : \", prediction)\n\n    image = img.imread(test_image)\n    plt.imshow(image)\n    plt.title(prediction)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:08:49.788022Z","iopub.execute_input":"2024-01-11T23:08:49.788685Z","iopub.status.idle":"2024-01-11T23:08:49.794865Z","shell.execute_reply.started":"2024-01-11T23:08:49.788654Z","shell.execute_reply":"2024-01-11T23:08:49.793827Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1001.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:08:50.218036Z","iopub.execute_input":"2024-01-11T23:08:50.218716Z","iopub.status.idle":"2024-01-11T23:08:56.914567Z","shell.execute_reply.started":"2024-01-11T23:08:50.218682Z","shell.execute_reply":"2024-01-11T23:08:56.913637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_101.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:09:10.969703Z","iopub.execute_input":"2024-01-11T23:09:10.97061Z","iopub.status.idle":"2024-01-11T23:09:11.548252Z","shell.execute_reply.started":"2024-01-11T23:09:10.970575Z","shell.execute_reply":"2024-01-11T23:09:11.547192Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1056.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:09:22.979433Z","iopub.execute_input":"2024-01-11T23:09:22.979799Z","iopub.status.idle":"2024-01-11T23:09:23.581891Z","shell.execute_reply.started":"2024-01-11T23:09:22.979773Z","shell.execute_reply":"2024-01-11T23:09:23.580897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1024.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T23:10:16.977954Z","iopub.execute_input":"2024-01-11T23:10:16.978336Z","iopub.status.idle":"2024-01-11T23:10:17.624113Z","shell.execute_reply.started":"2024-01-11T23:10:16.978306Z","shell.execute_reply":"2024-01-11T23:10:17.623189Z"},"trusted":true},"outputs":[],"execution_count":null}]}